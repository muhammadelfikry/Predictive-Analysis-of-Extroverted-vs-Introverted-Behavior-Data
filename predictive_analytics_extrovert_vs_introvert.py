# -*- coding: utf-8 -*-
"""Proyek Pertama : Predictive Analytics Machine Learning Terapan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RWvp0cxs6SDRJ3-Nx4UvbClp2IEuuKPo

## Import Library

Kode berikut digunakan untuk mengimpor berbagai pustaka yang diperlukan dalam pengembangan model machine learning untuk tugas klasifikasi. Setiap library memiliki fungsi khusus yang mendukung proses preprocessing data, pembuatan model, evaluasi, serta visualisasi hasil
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import MinMaxScaler
from google.colab import files
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""## Data loading

Pada tahapan ini, dataset akan diunduh dari Kaggle dan kemudian dibaca menggunakan pandas. Pandas merupakan library Python yang digunakan untuk memanipulasi dan menganalisis data dalam bentuk tabel (DataFrame).

### Get data from kaggle

Kode berikut digunakan untuk mengunggah berkas API key dari Kaggle (kaggle.json) dan mengunduh dataset yang diperlukan. Setelah proses pengunduhan selesai, dataset diekstrak (unzip) agar dapat dibaca menggunakan pandas.
"""

files.upload()

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets download rakeshkapilavai/extrovert-vs-introvert-behavior-data

! unzip extrovert-vs-introvert-behavior-data.zip

"""### Load Dataset

Setelah dataset berhasil diunduh dan diekstrak, langkah selanjutnya adalah membaca file dataset menggunakan fungsi .read_csv() dari library pandas. Fungsi ini digunakan untuk memuat data dari file CSV ke dalam bentuk DataFrame.
"""

data = pd.read_csv("/content/extrovert-vs-introvert-behavior-data.zip")
data.head()

"""## Exploratory Data Analysis

Pada tahapan ini dilakukan Exploratory Data Analysis (EDA) dengan tujuan untuk:
- Menganalisis karakteristik dari setiap fitur dalam dataset.

- Melakukan pemeriksaan terhadap adanya outlier yang dapat memengaruhi hasil model.
- Mengevaluasi tingkat korelasi antar fitur, terutama terhadap label target, guna memahami hubungan antar variabel dalam dataset.

### Variable Description

Mengecek informasi dasar pada dataset menggunakan fungsi info() untuk mengetahui tipe data, jumlah nilai non-null, serta memori yang digunakan oleh setiap kolom.
"""

data.info()

"""Mengecek ringkasan statistik dari data numerik pada dataset menggunakan fungsi describe(), yang menampilkan nilai seperti mean, standar deviasi, minimum, maksimum, dan kuartil."""

data.describe()

"""### Data Cleansing

Melakukan pemeriksaan terhadap missing value dan data duplikat pada dataset untuk memastikan kualitas data sebelum proses analisis dan pemodelan.
"""

print("Total missing value: ", data.isnull().sum().sum())
print("Total duplicate: ", data.duplicated().sum())

"""Kode berikut digunakan untuk membersihkan dataset dengan cara menghapus missing values (nilai yang hilang) dan data duplikat. Langkah ini penting agar data yang digunakan bersih dan tidak mengandung nilai yang dapat memengaruhi hasil analisis dan pemodelan."""

data.dropna(inplace=True)
data.drop_duplicates(inplace=True)
print("Total missing value: ", data.isnull().sum().sum())
print("Total duplicate: ", data.duplicated().sum())

"""### Outlier Detection

Kode berikut digunakan untuk melakukan visualisasi menggunakan boxplot dari library seaborn guna mendeteksi keberadaan outlier pada setiap fitur numerik dalam dataset. Dari hasil visualisasi boxplot, tidak ditemukan adanya outlier pada fitur-fitur numerik, sehingga tidak diperlukan tahapan pembersihan data lebih lanjut terkait outlier.
"""

plt.figure(figsize=(10, 6))
plt.title("Outlier Detection")
sns.boxplot(data=data)
plt.tight_layout()
plt.show()

"""### Univariate Analysis

Pada tahap ini dilakukan Univariate analysis untuk memahami distribusi dan karakteristik masing-masing fitur secara terpisah.

Kode berikut digunakan untuk melihat frekuensi dan proporsi masing-masing kategori dengan menggunakan visualisasi count plot. Analisis ini membantu dalam memahami jumlah data pada setiap kategori serta mendeteksi adanya ketidakseimbangan kelas.
"""

cat_features = data.select_dtypes(include="object").columns

for feature in cat_features:
  count = data[feature].value_counts()
  df = pd.DataFrame({
      "Count": count,
      "Percentage": round(count/np.sum(count) * 100, 1)
  })
  print(df, "\n")
  plt.figure(figsize=(8, 6))
  sns.countplot(x=feature, data=data, hue=feature, palette="viridis")
  plt.title(f"{feature} Distribution")
  plt.xticks(rotation=45)
  plt.tight_layout()
  plt.show()

"""Visualisasi berikut menggunakan fungsi .hist() dari pandas untuk menampilkan histogram fitur numerik dalam dataset. Histogram membantu memahami distribusi data, seperti pola sebaran, kecenderungan sentral, dan potensi keberadaan skewness atau multimodalitas pada setiap fitur."""

data.hist(bins=50, figsize=(15, 10))
plt.tight_layout()
plt.show()

"""### Multivariate Analysis

Pada tahapan ini dilakukan multivariate analysis untuk melihat hubungan antar fitur dalam dataset.

Kode berikut digunakan untuk menganalisis dan memvisualisasikan hubungan antara fitur numerik dengan label target. Hal ini membantu memahami bagaimana nilai fitur memengaruhi kelas atau kategori target dalam dataset.
"""

num_features = data.select_dtypes(include=np.number).columns

plt.figure(figsize=(15, 10))
for i, feature in enumerate(num_features, 1):
  plt.subplot(3, 2, i)
  sns.barplot(x="Personality", y=feature, data=data, hue="Personality", palette="viridis")
  plt.title(f"{feature} by Personality")
  plt.xlabel(None)
plt.tight_layout()
plt.show()

"""Plot ini membantu mengidentifikasi pola, korelasi, dan distribusi fitur yang berbeda antar kelas secara visual, sehingga memudahkan pemahaman tentang bagaimana fitur-fitur tersebut berinteraksi dan berkontribusi pada klasifikasi."""

plt.figure(figsize=(15, 10))
sns.pairplot(data, diag_kind="kde", hue="Personality", palette="viridis")
plt.tight_layout()

plt.show()

"""Visualisasi heatmap digunakan untuk menampilkan matriks korelasi antar fitur numerik dalam dataset. Analisis ini membantu mengidentifikasi fitur-fitur yang saling berkaitan erat sehingga dapat dipertimbangkan dalam pemilihan fitur untuk model."""

plt.figure(figsize=(12,10))
sns.heatmap(data[num_features].corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

"""## Data Preperation

Pada tahap ini, dataset dipersiapkan agar siap digunakan dalam proses pemodelan machine learning. Beberapa langkah penting yang dilakukan antara lain:

- Pembersihan Data
- Encoding Kategori
- Pembagian Dataset
- Feature Scaling

### Encoding Feature

Kode berikut digunakan untuk melakukan proses encoding yang mengubah fitur kategori menjadi representasi numerik. Teknik yang digunakan adalah one-hot encoding, yang membuat kolom baru untuk setiap kategori unik sehingga model machine learning dapat memproses data tersebut dengan lebih baik.
"""

feature_to_encode = ["Stage_fear", "Drained_after_socializing"]

data_encoding = pd.get_dummies(data, columns=feature_to_encode)
data_encoding.head()

"""### Split Dataset

Pada tahapan ini, dataset dibagi menjadi tiga bagian yaitu train, validation, dan test set menggunakan fungsi train_test_split(). Pembagian ini bertujuan untuk melatih model pada data training, melakukan penyesuaian parameter pada data validation, serta menguji performa akhir model pada data test secara objektif.
"""

x = data_encoding.drop(["Personality"], axis=1)
y = data_encoding["Personality"]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)

print("Shape of X_train: ", X_train.shape)
print("Shape of X_test: ", X_test.shape)
print("Shape of X_val: ", X_val.shape)
print("Shape of y_train: ", y_train.shape)
print("Shape of y_test: ", y_test.shape)
print("Shape of y_val: ", y_val.shape)

"""### Normalization

Kode berikut berguna untuk normalization menggunakan MinMaxScaler yang mengubah mengubah skala fitur numerik agar nilainya berada dalam rentang 0 hingga 1. Proses ini penting agar setiap fitur memiliki kontribusi yang seimbang saat pelatihan model, menghindari dominasi fitur dengan nilai besar dan meningkatkan konvergensi algoritma machine learning.
"""

min_max_scaler = MinMaxScaler()

X_train = min_max_scaler.fit_transform(X_train)
X_test = min_max_scaler.transform(X_test)
X_val = min_max_scaler.transform(X_val)

"""## Model Development

Pada tahap ini dilakukan pengembangan model dengan menggunakan tiga algoritma berbeda, yaitu K-Nearest Neighbors (KNN), Decision Tree, dan Support Vector Machine (SVM). Pendekatan ini bertujuan untuk membandingkan performa masing-masing model dalam menyelesaikan masalah klasifikasi yang dihadapi.

### Model Training

Kode berikut digunakan untuk menginisiasi tiga model machine learning yang akan digunakan dalam pengembangan, yaitu K-Nearest Neighbors dengan 10 tetangga terdekat, Decision Tree dengan pengaturan random state untuk reproducibility, dan Support Vector Machine juga dengan random state yang sama.
"""

models = {
    "KNN": KNeighborsClassifier(n_neighbors=10),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "SVM": SVC(random_state=42)
}

"""Model dilatih menggunakan data training untuk memperoleh nilai akurasi pada data training (train_accuracy) dan data validasi (val_accuracy). Hasil evaluasi tersebut kemudian disimpan dalam sebuah DataFrame bernama models_result untuk memudahkan perbandingan performa antar model."""

models_result = pd.DataFrame(index=["KNN", "Decision Tree", "SVM"],
                             columns=["train_accuracy", "val_accuracy"])

for name, model in models.items():
  model.fit(X_train, y_train)
  y_pred_train = model.predict(X_train)
  y_pred_test = model.predict(X_val)
  models_result.loc[name, "train_accuracy"] = accuracy_score(y_train, y_pred_train)
  models_result.loc[name, "val_accuracy"] = accuracy_score(y_val, y_pred_test)

models_result

"""Kode berikut digunakan untuk memvisualisasikan hasil akurasi dari masing-masing model yang tersimpan dalam DataFrame models_result. Grafik batang ini memperlihatkan perbandingan akurasi pada data training dan validasi."""

fig, ax = plt.subplots(figsize=(8, 6))
models_result.sort_values(by="val_accuracy", ascending=True).plot(kind="barh", ax=ax)
ax.set_title("Model Accuracy Comparison")
ax.set_xlabel("Accuracy")
ax.set_ylabel(None)
plt.tight_layout()
plt.show()

"""### Hyperparameter Tuning

Pada tahap ini dilakukan hyperparameter tuning pada model terbaik menggunakan GridSearchCV. Proses ini mencari kombinasi parameter terbaik dari beberapa kandidat yang ditentukan, dengan menggunakan validasi silang (cross-validation) untuk meningkatkan performa model dan menghindari overfitting.
"""

param_grid = {
    "C": [0.1, 1, 10, 100],
    "gamma": ["auto", "scale", 0.01, 0.001],
    "kernel": ["linear", "rbf", "poly"]
}

grid_search = GridSearchCV(
    models["SVM"],
    param_grid,
    cv=5,
    scoring="accuracy",
    verbose=2,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
print("Best Parameters:", best_params)

best_model = grid_search.best_estimator_

grid_search_score = best_model.score(X_val, y_val)
print("Grid Search Accuracy Score:", grid_search_score)

"""### Model Evaluation

Kode berikut digunakan untuk melakukan prediksi menggunakan model terbaik pada data testing. Selanjutnya, hasil prediksi dievaluasi dengan menggunakan classification report untuk melihat metrik seperti precision, recall, dan f1-score, serta confusion matrix yang divisualisasikan dengan heatmap untuk memudahkan analisis kesalahan klasifikasi.
"""

y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)

print(classification_report(y_test, y_pred_test))
cm = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()